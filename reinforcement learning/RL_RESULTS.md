# RL Results Guide - Where to Find Everything

This guide shows you **where all your RL training results are stored** and **how to view them**.

---

## ğŸ“ File Structure - Where Results Are Stored

```
Your Project/
â”œâ”€â”€ rl/checkpoints/                    # Trained models & training state
â”‚   â”œâ”€â”€ dqn_episode_10.pt             # Checkpoint at episode 10
â”‚   â”œâ”€â”€ dqn_episode_20.pt             # Checkpoint at episode 20
â”‚   â”œâ”€â”€ dqn_final.pt                  # Final trained model
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ results/rl/                        # Visualizations & analysis
â”‚   â”œâ”€â”€ training_curves.png           # Episode rewards, objectives, loss
â”‚   â”œâ”€â”€ training_analysis.png         # Comprehensive analysis plots
â”‚   â”œâ”€â”€ training_results.json         # Exported statistics (JSON)
â”‚   â”œâ”€â”€ episode_rewards.csv           # Episode rewards (CSV)
â”‚   â””â”€â”€ training_losses.csv           # Training losses (CSV)
â”‚
â””â”€â”€ results/                           # Design optimization results
    â”œâ”€â”€ design_history.pkl            # All designs evaluated
    â””â”€â”€ (other reports from main project)
```

---

## ğŸ” How to View Results

### Method 1: **RL Results Viewer** (Recommended)

The **`rl_results_viewer.py`** script provides comprehensive analysis of all results:

#### View Latest Training Results
```bash
python rl_results_viewer.py
```
Shows:
- Training progress (epsilon, steps, episodes)
- Reward statistics (best, worst, average)
- Loss statistics (current, average, min, max)
- Network info
- **Automatic training plots**

#### List All Checkpoints
```bash
python rl_results_viewer.py --list
```
Shows all saved checkpoints with dates and sizes

#### View Specific Checkpoint
```bash
python rl_results_viewer.py --checkpoint rl/checkpoints/dqn_episode_30.pt
```

#### Compare Multiple Checkpoints
```bash
python rl_results_viewer.py --compare rl/checkpoints/dqn_episode_10.pt rl/checkpoints/dqn_final.pt
```
Side-by-side comparison of training progress

#### Show Best Designs Found
```bash
python rl_results_viewer.py --designs
```
Displays top 5 best hardware designs discovered

#### Export Results to JSON
```bash
python rl_results_viewer.py --export json
```
Creates `results/rl/training_results.json`

#### Export Results to CSV
```bash
python rl_results_viewer.py --export csv
```
Creates:
- `results/rl/episode_rewards.csv`
- `results/rl/training_losses.csv`

#### Show Training Plots Only
```bash
python rl_results_viewer.py --plot
```

---

### Method 2: **During Training** (Live Results)

When you run training, results are displayed in real-time:

```bash
python run_dqn_quick.py
```

You'll see live updates like:
```
Episode 1/10
  Iter  1: PAR= 4, BD=1024, Cells= 856, Obj= 1070.0, Reward= -8.7, Loss=0.0234
  Iter  5: PAR=16, BD= 512, Cells=1234, Obj= 1311.2, Reward=-11.1, Loss=0.0189
  
  ğŸ‰ NEW GLOBAL BEST! Objective: 892.3
  
Episode Summary:
  Total Reward: -234.56
  Best Objective: 892.3
  Designs Explored: 10
```

At the end:
```
ğŸ† OPTIMIZATION COMPLETE

âœ¨ Best Design Found:
   PAR:                  8
   BUFFER_DEPTH:         1024

ğŸ“Š Best Metrics:
   Total Cells:          892
   Flip-Flops:           234
   Throughput:           8 ops/cycle
   Area Efficiency:      111.5 cells/op
   Objective Score:      892.3

âœ… Best design meets all constraints!
```

---

### Method 3: **Load Checkpoint Programmatically**

Access checkpoint data in Python:

```python
import torch

# Load checkpoint
checkpoint = torch.load('rl/checkpoints/dqn_final.pt', map_location='cpu')

# Access training data
episode_rewards = checkpoint['episode_rewards']
losses = checkpoint['losses']
epsilon = checkpoint['epsilon']
steps = checkpoint['steps_done']

# Print statistics
print(f"Total episodes: {len(episode_rewards)}")
print(f"Best reward: {max(episode_rewards)}")
print(f"Current exploration rate: {epsilon}")
```

---

### Method 4: **View Visualizations**

Training automatically generates plots:

1. **Training Curves** (`results/rl/training_curves.png`)
   - Episode rewards over time
   - Best objective per episode
   - Training loss progression

2. **Training Analysis** (`results/rl/training_analysis.png`)
   - Generated by `rl_results_viewer.py`
   - Comprehensive 6-plot analysis:
     - Episode rewards (raw + smoothed)
     - Cumulative rewards
     - Training loss (log scale)
     - Reward distribution
     - Loss distribution
     - Episode-to-episode improvement

**View with any image viewer or:**
```python
from PIL import Image
import matplotlib.pyplot as plt

img = Image.open('results/rl/training_curves.png')
plt.imshow(img)
plt.axis('off')
plt.show()
```

---

## ğŸ“Š What Each Result File Contains

### **Checkpoints** (`rl/checkpoints/*.pt`)

PyTorch checkpoint files containing:
```python
{
    'policy_net': {...},          # Neural network weights
    'target_net': {...},          # Target network weights
    'optimizer': {...},           # Optimizer state
    'epsilon': 0.15,              # Current exploration rate
    'steps_done': 1234,           # Total training steps
    'episode_rewards': [...],     # Reward per episode
    'losses': [...]               # Loss per training step
}
```

**Use for:**
- Resume training
- Evaluate trained agent
- Analyze learning progress
- Compare different training runs

---

### **Training Results JSON** (`results/rl/training_results.json`)

Exported training statistics:
```json
{
  "timestamp": "2024-01-15T10:30:00",
  "training_progress": {
    "epsilon": 0.15,
    "steps_done": 1234,
    "episodes_completed": 50
  },
  "statistics": {
    "episode_rewards": {
      "values": [...],
      "best": 45.2,
      "worst": -120.5,
      "mean": -12.3,
      "std": 25.6,
      "median": -8.9
    },
    "losses": {
      "values": [...],
      "current": 0.0234,
      "mean": 0.0456,
      "std": 0.0123,
      "min": 0.0012,
      "max": 0.234
    }
  }
}
```

**Use for:**
- Reporting
- Data analysis
- Integration with other tools
- Archiving results

---

### **CSV Files** (`results/rl/*.csv`)

**episode_rewards.csv:**
```csv
episode,reward
1,-45.2
2,-38.1
3,-32.5
...
```

**training_losses.csv:**
```csv
step,loss
1,0.234
2,0.198
3,0.156
...
```

**Use for:**
- Spreadsheet analysis (Excel, Google Sheets)
- Data science workflows (pandas, R)
- Custom plotting
- Statistical analysis

---

## ğŸ¯ Common Use Cases

### **Check if Training is Working**
```bash
python rl_results_viewer.py
```
Look for:
- âœ… Episode rewards increasing
- âœ… Loss stabilizing/decreasing
- âœ… Epsilon decaying towards 0.05

### **Find Best Design**
```bash
python rl_results_viewer.py --designs
```
Shows top 5 designs with full metrics

### **Compare Training Progress**
```bash
python rl_results_viewer.py --compare \
    rl/checkpoints/dqn_episode_10.pt \
    rl/checkpoints/dqn_episode_30.pt \
    rl/checkpoints/dqn_final.pt
```
See how agent improved over training

### **Export for Report/Paper**
```bash
# Export to CSV for Excel/analysis
python rl_results_viewer.py --export csv

# Export to JSON for documentation
python rl_results_viewer.py --export json

# Generate high-res plots
python rl_results_viewer.py --plot
# Plots saved to results/rl/
```

### **Troubleshoot Training Issues**
```bash
python rl_results_viewer.py --checkpoint rl/checkpoints/dqn_final.pt
```
Check:
- Is epsilon too high? (should be ~0.05-0.15 by end)
- Are rewards improving?
- Is loss exploding? (should be stable, <0.1)
- How many episodes completed?

---

## ğŸ“ˆ Reading the Results

### **Episode Rewards**
- **Negative values are normal** (objective penalties)
- **Higher = Better** (less negative)
- **Look for upward trend** over episodes
- **Last 10 episodes** should be better than first 10

### **Training Loss**
- **Lower = Better** (but not always)
- **Should stabilize** (not oscillate wildly)
- **Typical range**: 0.01 - 0.1
- **If >1.0**: training may be unstable

### **Epsilon (Exploration Rate)**
- **Starts at 1.0** (100% random)
- **Decays to ~0.05** (5% random)
- **Check current value** to see exploration/exploitation balance

### **Best Objective**
- **Lower = Better** (minimize area-efficiency product)
- **Compare with LLM agent** (run `main.py`)
- **Check constraints** (should be satisfied)

---

## ğŸ”§ Advanced: Programmatic Access

### Load and Analyze Results in Python

```python
import torch
import numpy as np
import matplotlib.pyplot as plt

# Load checkpoint
checkpoint = torch.load('rl/checkpoints/dqn_final.pt')

# Get data
rewards = checkpoint['episode_rewards']
losses = checkpoint['losses']

# Analyze
print(f"Training completed: {len(rewards)} episodes")
print(f"Final reward: {rewards[-1]:.2f}")
print(f"Improvement: {rewards[-1] - rewards[0]:.2f}")
print(f"Best reward: {max(rewards):.2f}")

# Plot custom analysis
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(rewards)
plt.title('Episode Rewards')
plt.xlabel('Episode')
plt.ylabel('Reward')

plt.subplot(1, 2, 2)
plt.plot(losses[-500:])  # Last 500 steps
plt.title('Recent Training Loss')
plt.xlabel('Step')
plt.ylabel('Loss')

plt.tight_layout()
plt.savefig('my_custom_analysis.png')
plt.show()
```

### Load Agent and Inspect Q-values

```python
from rl.dqn_agent import DQNAgent
import numpy as np

# Create and load agent
agent = DQNAgent()
agent.load('rl/checkpoints/dqn_final.pt')

# Inspect learned policy
state = np.zeros(16)  # Initial state
action_idx, params = agent.select_action(state, evaluation=True)

print(f"Agent would choose: PAR={params['PAR']}, BUFFER_DEPTH={params['BUFFER_DEPTH']}")

# Check Q-values for all actions
import torch
state_tensor = torch.FloatTensor(state).unsqueeze(0)
q_values = agent.policy_net(state_tensor)
print(f"\nQ-values for all 24 actions:")
for i, q in enumerate(q_values[0]):
    print(f"  Action {i} (PAR={agent.actions[i]['PAR']}, BD={agent.actions[i]['BUFFER_DEPTH']}): {q.item():.3f}")
```

---

## ğŸ†˜ Troubleshooting Results

### "No checkpoint directory found"
**Solution:** Train the agent first:
```bash
python run_dqn_quick.py
```

### "No checkpoints found"
**Solution:** Training hasn't completed. Check if training is running or completed.

### "Empty plots"
**Solution:** Not enough training data. Train for at least 5 episodes.

### "Cannot load checkpoint"
**Solution:** 
1. Check PyTorch is installed: `pip install torch`
2. Verify file exists: `ls rl/checkpoints/`
3. Check file isn't corrupted (try different checkpoint)

---

## ğŸ“š Summary

**Where Results Are:**
- âœ… Checkpoints: `rl/checkpoints/*.pt`
- âœ… Plots: `results/rl/*.png`
- âœ… Exports: `results/rl/*.json`, `results/rl/*.csv`
- âœ… Console: Live output during training

**How to View:**
- âœ… **Best way**: `python rl_results_viewer.py`
- âœ… During training: Watch console output
- âœ… After training: Open `results/rl/training_curves.png`
- âœ… Programmatic: Load `.pt` files with PyTorch

**What to Check:**
- âœ… Episode rewards improving
- âœ… Loss stabilizing
- âœ… Best designs meeting constraints
- âœ… Epsilon decaying properly

---

**Questions?** Check the comprehensive viewer:
```bash
python rl_results_viewer.py --help
```

**Happy analyzing! ğŸ“ŠğŸš€**
